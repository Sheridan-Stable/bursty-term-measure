{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIKGXBuV_ryc"
   },
   "source": [
    "# GENIA preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrjI0AVxAPrd"
   },
   "source": [
    "## preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExyVoNCjAY_s",
    "outputId": "b2aa133f-0b23-4eff-9bcf-18bd4c6976ee"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import inflect\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwwOh2AFAzx4"
   },
   "source": [
    "## Define Various NLP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Iuh4UvsPA6Gy"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    return words\n",
    "\n",
    "def take_out_str(arr, s):\n",
    "  \"\"\"Goes through an array of strings and removes substring s from every\n",
    "  element in the array\"\"\"\n",
    "  for i in range(len(arr)):\n",
    "    if type(arr[i]) is str:\n",
    "      if s in arr[i]:\n",
    "        arr[i] = arr[i].replace(s, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVmqBvmVDaLS"
   },
   "source": [
    "## Read in GENIA corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8gjCeThsDdW7"
   },
   "outputs": [],
   "source": [
    "xml_file_path = '../0-data-raw/GENIAcorpus3.02.xml'\n",
    "infile = open(xml_file_path, 'r')\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents, 'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LCy1myFA8Ua"
   },
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY7xO3ziA_HV"
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "In here, we eliminate tags, or other elements in the data set that might be troublesome. These are:\n",
    "\n",
    "\n",
    "\n",
    "*   cons tags without a sem attribute\n",
    "    *  Note that they're all deleted upon deleting the coordinated con tags with a coordinated lex attribute\n",
    "*   All the coordinated lex attributes\n",
    "*   All instances of the words \"(ABSTRACT TRUNCATED AT 250 WORDS)\"\n",
    "*   cons tags without a lex attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuQY8Cq7CTS3",
    "outputId": "60a9ec7d-d204-4e5f-c156-1965d31fbb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There are a total of  90969  cons tags in the collection.\n",
      "There are  0 cons tags without a sem attribute.\n",
      "The proportion of cons tags with no sem is  0.0\n",
      "There are  0 cons tags without a lex attribute.\n",
      "The proportion of cons tags with no lex is  0.0\n",
      "There are  0 cons tags with a coordinated sem attribute.\n",
      "The proportion of cons tags with coordinated sems is  0.0\n",
      "There are a total of  0 sentences sentences which solely contain the phrase '(ABSTRACT TRUNCATED AT 250 WORDS)'\n",
      "The proportion of sentences that contain said phrase is  0.0\n",
      "There are a total of  0 sentences sentences which solely contain the phrase '(ABSTRACT TRUNCATED AT 400 WORDS)'\n",
      "The proportion of sentences that contain said phrase is  0.0\n"
     ]
    }
   ],
   "source": [
    "all_cons = soup.find_all('cons')\n",
    "total_cons = len(all_cons)\n",
    "all_cons = np.array(all_cons, dtype=object)\n",
    "no_sem = soup.find_all('cons', sem=False)\n",
    "no_sem_total = len(no_sem)\n",
    "\n",
    "coordinated_arr = []\n",
    "for i in range(len(all_cons)):\n",
    "    if 'sem' in all_cons[i].attrs:\n",
    "      if '(' in all_cons[i].attrs['sem']:\n",
    "        coordinated_arr.append(all_cons[i])\n",
    "\n",
    "total_coordinated = len(coordinated_arr)\n",
    "for i in range(total_coordinated):\n",
    "  coordinated_arr[i].decompose()\n",
    "\n",
    "\n",
    "no_lex = soup.find_all('cons', lex=False)\n",
    "print(no_lex)\n",
    "no_lex_total = len(no_lex)\n",
    "for lex in no_lex:\n",
    "  lex.decompose()\n",
    "\n",
    "truncated_instances = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 250 WORDS)')\n",
    "truncated_total = len(truncated_instances)\n",
    "for trunc in truncated_instances:\n",
    "  trunc.decompose()\n",
    "\n",
    "truncated_instances400 = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 400 WORDS)')\n",
    "truncated_total400 = len(truncated_instances400)\n",
    "for trunc in truncated_instances400:\n",
    "  trunc.decompose()\n",
    "    \n",
    "print('There are a total of ', total_cons, ' cons tags in the collection.')\n",
    "print('There are ', no_sem_total, 'cons tags without a sem attribute.')\n",
    "print('The proportion of cons tags with no sem is ', no_sem_total/total_cons)\n",
    "print('There are ', no_lex_total, 'cons tags without a lex attribute.')\n",
    "print('The proportion of cons tags with no lex is ', no_lex_total/total_cons)\n",
    "print('There are ', total_coordinated, 'cons tags with a coordinated sem attribute.')\n",
    "print('The proportion of cons tags with coordinated sems is ', total_coordinated/total_cons)\n",
    "print('There are a total of ', truncated_total, 'sentences sentences which solely contain the phrase \\'(ABSTRACT TRUNCATED AT 250 WORDS)\\'')\n",
    "print('The proportion of sentences that contain said phrase is ', truncated_total/total_cons)\n",
    "print('There are a total of ', truncated_total400, 'sentences sentences which solely contain the phrase \\'(ABSTRACT TRUNCATED AT 400 WORDS)\\'')\n",
    "print('The proportion of sentences that contain said phrase is ', truncated_total400/total_cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAJ1M4_8KHTQ"
   },
   "source": [
    "The following is a test that the above code cell worked. If it worked, all quantities printed (except for the total number of con tags) should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ0xDwVBKPgK",
    "outputId": "54e7ce71-4446-49f5-aa29-8479f9307de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of con tags now:  90969\n",
      "Total of con tags without sem attribute now:  0\n",
      "Total of con tags without lex attribute now:  0\n",
      "Total instances of truncated abstracts (at 250) now:  0\n",
      "Total instances of truncated abstracts (at 400) now:  0\n",
      "Total instances of cons tags with coordinated sem attributes:  0\n"
     ]
    }
   ],
   "source": [
    "cons = soup.find_all('cons')\n",
    "new_total_cons = len(cons)\n",
    "\n",
    "new_no_sem = soup.find_all('cons', sem=False)\n",
    "new_no_sem_total = len(new_no_sem)\n",
    "\n",
    "new_no_lex = soup.find_all('cons', lex=False)\n",
    "new_no_lex_total = len(new_no_lex)\n",
    "\n",
    "new_truncated_instances = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 250 WORDS)')\n",
    "new_truncated_total = len(new_truncated_instances)\n",
    "\n",
    "new_truncated_instances400 = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 400 WORDS)')\n",
    "new_truncated_total400 = len(new_truncated_instances400)\n",
    "\n",
    "new_coordinated_counter = 0\n",
    "for i in range(new_total_cons):\n",
    "  if 'sem' in cons[i].attrs:\n",
    "    if '(' in cons[i].attrs['sem']:\n",
    "      new_coordinated_counter += 1\n",
    "\n",
    "print('Total of con tags now: ', new_total_cons)\n",
    "print('Total of con tags without sem attribute now: ', new_no_sem_total)\n",
    "print('Total of con tags without lex attribute now: ', new_no_sem_total)\n",
    "print('Total instances of truncated abstracts (at 250) now: ', new_truncated_total)\n",
    "print('Total instances of truncated abstracts (at 400) now: ', new_truncated_total400)\n",
    "print('Total instances of cons tags with coordinated sem attributes: ', new_coordinated_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfoX86nZMpwu"
   },
   "source": [
    "## Output Files Creation\n",
    "If everything goes accordingly, three files should be written:\n",
    "\n",
    "* A csv file containing the document ID'S and their respective doc index\n",
    "* A tsv file containing a mapping from each lex to each sem\n",
    "* A json file containing the preprocessed GENIA collection. Each element in this json file is a list of strings. Some are keywords (if they are in the lex list), some aren't. The words won't be in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "LQg2xQJdTCbq",
    "outputId": "a81d4ac5-151b-4afc-ef1e-da265f0116f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MEDLINE:95333264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MEDLINE:95343554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MEDLINE:95347379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MEDLINE:95280913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>MEDLINE:96011839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>MEDLINE:96009598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>MEDLINE:95403454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>MEDLINE:95385995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                ID\n",
       "0         0  MEDLINE:95369245\n",
       "1         1  MEDLINE:95333264\n",
       "2         2  MEDLINE:95343554\n",
       "3         3  MEDLINE:95347379\n",
       "4         4  MEDLINE:95280913\n",
       "...     ...               ...\n",
       "1995   1995  MEDLINE:96011839\n",
       "1996   1996  MEDLINE:96009598\n",
       "1997   1997  MEDLINE:95403454\n",
       "1998   1998  MEDLINE:95385995\n",
       "1999   1999  MEDLINE:95370270\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the text in the bibliomisc tags\n",
    "doc_id_tags = soup.find_all('bibliomisc')\n",
    "doc_ids = [id.get_text() for id in doc_id_tags]\n",
    "\n",
    "# Write the csv file with the doc ids\n",
    "genia_doc_id_file_path = '../0-data-preprocessed/GENIAcorpus3.02-doc-ids.csv'\n",
    "index = range(len(doc_ids))\n",
    "doc_ids_df = pd.DataFrame({'index': index, 'ID': doc_ids})\n",
    "# Uncomment the line bellow to write the file\n",
    "doc_ids_df.to_csv(genia_doc_id_file_path, index=False)\n",
    "display(doc_ids_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WvHL68P3UDwh",
    "outputId": "50a579ed-2775-4787-8420-853f3c28d22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IL-2 receptor alpha\n",
      "IL-2R alpha\n",
      "CD30 ligand\n",
      "proteolytic digestion\n",
      "cytoplasmic component\n",
      "IL-7R alpha\n",
      "IL-7R alpha\n",
      "HSP70B promoter\n"
     ]
    }
   ],
   "source": [
    "lex_and_sem = []\n",
    "for con in cons:\n",
    "  lex_and_sem.append(con.attrs)\n",
    "\n",
    "lex_and_sem_df = pd.DataFrame(lex_and_sem)\n",
    "\n",
    "# Replacing lex's with spaces in between and adding '_lex' postfix\n",
    "for i in range(len(lex_and_sem_df['lex'])):\n",
    "    if ' ' in lex_and_sem_df['lex'][i]:\n",
    "        print(lex_and_sem_df['lex'][i])\n",
    "        lex_and_sem_df['lex'][i] = lex_and_sem_df['lex'][i].replace(' ', '_')\n",
    "    lex_and_sem_df['lex'][i] += '_lex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL-2_gene_expression_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL-2_gene_lex</td>\n",
       "      <td>G#DNA_domain_or_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF-kappa_B_activation_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF-kappa_B_lex</td>\n",
       "      <td>G#protein_molecule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CD28_lex</td>\n",
       "      <td>G#protein_molecule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31782</th>\n",
       "      <td>gp160-induced_AP-1_complex_lex</td>\n",
       "      <td>G#protein_complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31783</th>\n",
       "      <td>protein_synthesis-independent_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31784</th>\n",
       "      <td>calcium_channel_blocker_lex</td>\n",
       "      <td>G#other_organic_compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31785</th>\n",
       "      <td>anti-CD3-induced_interleukin-2_secretion_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31786</th>\n",
       "      <td>T_cell_unresponsiveness_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31787 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                lex                       sem\n",
       "0                          IL-2_gene_expression_lex              G#other_name\n",
       "1                                     IL-2_gene_lex    G#DNA_domain_or_region\n",
       "2                         NF-kappa_B_activation_lex              G#other_name\n",
       "3                                    NF-kappa_B_lex        G#protein_molecule\n",
       "4                                          CD28_lex        G#protein_molecule\n",
       "...                                             ...                       ...\n",
       "31782                gp160-induced_AP-1_complex_lex         G#protein_complex\n",
       "31783             protein_synthesis-independent_lex              G#other_name\n",
       "31784                   calcium_channel_blocker_lex  G#other_organic_compound\n",
       "31785  anti-CD3-induced_interleukin-2_secretion_lex              G#other_name\n",
       "31786                   T_cell_unresponsiveness_lex              G#other_name\n",
       "\n",
       "[31787 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lex_and_sem_df = lex_and_sem_df.drop_duplicates(subset=['lex'])\n",
    "lex_and_sem_df = lex_and_sem_df.reset_index(drop=True)\n",
    "\n",
    "unique_lex = np.array(lex_and_sem_df['lex'])\n",
    "new_lex_col = take_out_str(lex_and_sem_df['lex'], '\\\"')\n",
    "lex_and_sem_df = lex_and_sem_df.replace(lex_and_sem_df['lex'], new_lex_col)\n",
    "# Write semantic classes to tsv (uncomment to write file)\n",
    "genia_keywords_file_path = '../0-data-preprocessed/GENIAcorpus3.02-keywords.tsv'\n",
    "lex_and_sem_df.to_csv(genia_keywords_file_path, index=False, sep='\\t')\n",
    "\n",
    "display(lex_and_sem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jurdkpC75H3X"
   },
   "outputs": [],
   "source": [
    "# Get the json file\n",
    "xml_articles = soup.find_all('article')\n",
    "\n",
    "joint_descendants = []\n",
    "for article in xml_articles:\n",
    "  joint_descendants.append([descendant for descendant in article.descendants])\n",
    "\n",
    "j_sen = []\n",
    "for i in range(len(joint_descendants)):\n",
    "  j_sen.append([])\n",
    "  for sen in joint_descendants[i]:\n",
    "    if sen.name == 'sentence':\n",
    "      j_sen[i].append(sen)\n",
    "\n",
    "in_sen = []\n",
    "for i in range(len(j_sen)):\n",
    "  in_sen.append([])\n",
    "  for sentence in j_sen[i]:\n",
    "    in_sen[i].append([descendant for descendant in sentence.descendants])\n",
    "\n",
    "in_sentences = []\n",
    "for i in range(len(in_sen)):\n",
    "  in_sentences.append([])\n",
    "  for j in range(len(in_sen[i])):\n",
    "    for k in range(len(in_sen[i][j])):\n",
    "      in_sentences[i].append(in_sen[i][j][k])\n",
    "\n",
    "bag_of_content = []\n",
    "for i in range(len(in_sentences)):\n",
    "  bag_of_content.append([])\n",
    "  for j in range(len(in_sentences[i])):\n",
    "    if type(in_sentences[i][j]) is bs4.element.Tag:\n",
    "      bag_of_content[i].append(in_sentences[i][j].attrs)\n",
    "    elif in_sentences[i][j].parent.name == 'sentence':\n",
    "      bag_of_content[i].append(in_sentences[i][j])\n",
    "\n",
    "keywords = []\n",
    "stopwords = []\n",
    "for i in range(len(bag_of_content)):\n",
    "  keywords.append([])\n",
    "  stopwords.append([])\n",
    "  for j in range(len(bag_of_content[i])):\n",
    "    if type(bag_of_content[i][j]) is dict:\n",
    "        keywords[i].append(bag_of_content[i][j]['lex'])\n",
    "    else:\n",
    "      stopwords[i].append(bag_of_content[i][j])\n",
    "\n",
    "for i in range(len(keywords)):\n",
    "    for j in range(len(keywords[i])):\n",
    "        if ' ' in keywords[i][j]:\n",
    "            keywords[i][j] = keywords[i][j].replace(' ', '_')\n",
    "        keywords[i][j] += '_lex'\n",
    "\n",
    "\n",
    "for i in range(len(stopwords)):\n",
    "  stopwords[i] = normalize(stopwords[i])\n",
    "  stopwords[i] = [word.strip() for word in stopwords[i]]\n",
    "  stopwords[i] = [word for word in stopwords[i] if len(word) > 0]\n",
    "\n",
    "combined = []\n",
    "for i in range(len(stopwords)):\n",
    "  combined.append(keywords[i] + stopwords[i])\n",
    "\n",
    "genia = []\n",
    "for doc in combined:\n",
    "  genia.append(' '.join(doc))\n",
    "\n",
    "take_out_str(genia, '\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "sIlPQEzwQalM",
    "outputId": "3ab07fa4-9e25-45c7-fc8f-ce4dfc0b1d36"
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell to write the file to the specified folder\n",
    "\n",
    "corpus_json_file_path = '../0-data-preprocessed/GENIAcorpus3.02-preprocessed.json'\n",
    "with open(corpus_json_file_path, 'w') as outfile:\n",
    "    json.dump(genia, outfile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
