{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIKGXBuV_ryc"
   },
   "source": [
    "# GENIA Data Preprocessing\n",
    "\n",
    "Authors: Samuel Sarria Hurtado and Paul Sheridan\n",
    "\n",
    "Last update: 2023-10-02\n",
    "\n",
    "Description: Preprocess the GENIA Term corpus version 3.02 dataset.\n",
    "\n",
    "Inputs:\n",
    "* Raw data (XML): GENIAcorpus3.02.xml\n",
    "\n",
    "Outputs:\n",
    "* Document Ids (CSV): GENIAcorpus3.02-doc-ids.csv\n",
    "* Lex/sem mapping (TSV): GENIAcorpus3.02-keywords.tsv\n",
    "* Preprocessed documents (JSON): GENIAcorpus3.02-preprocessed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrjI0AVxAPrd"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExyVoNCjAY_s",
    "outputId": "b2aa133f-0b23-4eff-9bcf-18bd4c6976ee"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import inflect\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwwOh2AFAzx4"
   },
   "source": [
    "## Load Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Iuh4UvsPA6Gy"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    return words\n",
    "\n",
    "def take_out_str(arr, s):\n",
    "  \"\"\"Goes through an array of strings and removes substring s from every\n",
    "  element in the array\"\"\"\n",
    "  for i in range(len(arr)):\n",
    "    if type(arr[i]) is str:\n",
    "      if s in arr[i]:\n",
    "        arr[i] = arr[i].replace(s, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LCy1myFA8Ua"
   },
   "source": [
    "## Preprocess the Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVmqBvmVDaLS"
   },
   "source": [
    "### Read in the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8gjCeThsDdW7"
   },
   "outputs": [],
   "source": [
    "infile_path = '../0-data-raw/GENIAcorpus3.02.xml'\n",
    "infile = open(infile_path, 'r')\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents, 'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY7xO3ziA_HV"
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Here we eliminate tags, and other elements in the dataset, that might be troublesome. These are:\n",
    "* Coordinated lex attributes.\n",
    "* Cons tags lacking a sem attribute. Note: that they're all deleted upon deleting the coordinated con tags with a coordinated lex attribute.\n",
    "* Cons tags without a lex attribute\n",
    "* All instances of the words \"(ABSTRACT TRUNCATED AT 250 WORDS)\".\n",
    "* All instances of the words \"(ABSTRACT TRUNCATED AT 400 WORDS)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuQY8Cq7CTS3",
    "outputId": "60a9ec7d-d204-4e5f-c156-1965d31fbb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cons: 97876\n",
      "Cons tags without sem attribute: 5154\n",
      "Proportion of cons tags without sem attribute: 0.05265846581388696\n",
      "Cons tags without lex attribute: 2\n",
      "Proportion of cons tags without lex attribute: 2.0434018554088846e-05\n",
      "Number of cons tags with a coordinated sem attribute: 1597\n",
      "Proportion of cons tags with a coordinated sem attribute: 0.016316563815439944\n",
      "Number of '(ABSTRACT TRUNCATED AT 250 WORDS)' instance: 24\n",
      "Associated proportion: 0.0002452082226490662\n",
      "Number of '(ABSTRACT TRUNCATED AT 400 WORDS)' instance: 3\n",
      "Associated proportion: 3.0651027831133274e-05\n"
     ]
    }
   ],
   "source": [
    "# Extract cons:\n",
    "all_cons = soup.find_all('cons')\n",
    "total_cons = len(all_cons)\n",
    "all_cons = np.array(all_cons, dtype=object)\n",
    "no_sem = soup.find_all('cons', sem=False)\n",
    "no_sem_total = len(no_sem)\n",
    "\n",
    "# Excise coordinated lex attributes:\n",
    "coordinated_arr = []\n",
    "for i in range(len(all_cons)):\n",
    "    if 'sem' in all_cons[i].attrs:\n",
    "      if '(' in all_cons[i].attrs['sem']:\n",
    "        coordinated_arr.append(all_cons[i])\n",
    "\n",
    "total_coordinated = len(coordinated_arr)\n",
    "for i in range(total_coordinated):\n",
    "  coordinated_arr[i].decompose()\n",
    "\n",
    "# Excise cons without lex attributes:\n",
    "no_lex = soup.find_all('cons', lex=False)\n",
    "no_lex_total = len(no_lex)\n",
    "for lex in no_lex:\n",
    "  lex.decompose()\n",
    "\n",
    "# Excise truncated at 250 eords abstract sentences:\n",
    "truncated_at_250_words_instances = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 250 WORDS)')\n",
    "truncated_at_250_words_total = len(truncated_at_250_words_instances)\n",
    "for trunc in truncated_at_250_words_instances:\n",
    "  trunc.decompose()\n",
    "\n",
    "# Excise truncated at 400 eords abstract sentences:\n",
    "truncated_at_400_words_instances = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 400 WORDS)')\n",
    "truncated_at_400_words_total = len(truncated_at_400_words_instances)\n",
    "for trunc in truncated_at_400_words_instances:\n",
    "  trunc.decompose()\n",
    "\n",
    "# Summarize results:\n",
    "print('Total number of cons:', total_cons)\n",
    "print('Cons tags without sem attribute:', no_sem_total)\n",
    "print('Proportion of cons tags without sem attribute:', no_sem_total/total_cons)\n",
    "print('Cons tags without lex attribute:', no_lex_total)\n",
    "print('Proportion of cons tags without lex attribute:', no_lex_total/total_cons)\n",
    "print('Number of cons tags with a coordinated sem attribute:', total_coordinated)\n",
    "print('Proportion of cons tags with a coordinated sem attribute:', total_coordinated/total_cons)\n",
    "print('Number of \\'(ABSTRACT TRUNCATED AT 250 WORDS)\\' instance:', truncated_at_250_words_total)\n",
    "print('Associated proportion:', truncated_at_250_words_total/total_cons)\n",
    "print('Number of \\'(ABSTRACT TRUNCATED AT 400 WORDS)\\' instance:', truncated_at_400_words_total)\n",
    "print('Associated proportion:', truncated_at_400_words_total/total_cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "Test that the above code cell worked. If it worked, all quantities printed (except for the total number of con tags) should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ0xDwVBKPgK",
    "outputId": "54e7ce71-4446-49f5-aa29-8479f9307de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of con tags now: 90969\n",
      "Total of con tags without sem attribute now: 0\n",
      "Total of con tags without lex attribute now: 0\n",
      "Total instances of abstracts truncated at 250 words now: 0\n",
      "Total instances of abstracts truncated at 400 words now: 0\n",
      "Total instances of cons tags with coordinated sem attributes: 0\n"
     ]
    }
   ],
   "source": [
    "cons = soup.find_all('cons')\n",
    "new_total_cons = len(cons)\n",
    "\n",
    "new_no_sem = soup.find_all('cons', sem=False)\n",
    "new_no_sem_total = len(new_no_sem)\n",
    "\n",
    "new_no_lex = soup.find_all('cons', lex=False)\n",
    "new_no_lex_total = len(new_no_lex)\n",
    "\n",
    "truncated_at_250_words_instances = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 250 WORDS)')\n",
    "new_truncated_at_250_words_total = len(truncated_at_250_words_instances)\n",
    "\n",
    "truncated_at_400_words_instances = soup.find_all('sentence', string='(ABSTRACT TRUNCATED AT 400 WORDS)')\n",
    "new_truncated_at_400_words_total = len(truncated_at_400_words_instances)\n",
    "\n",
    "new_coordinated_counter = 0\n",
    "for i in range(new_total_cons):\n",
    "  if 'sem' in cons[i].attrs:\n",
    "    if '(' in cons[i].attrs['sem']:\n",
    "      new_coordinated_counter += 1\n",
    "\n",
    "print('Total of con tags now:', new_total_cons)\n",
    "print('Total of con tags without sem attribute now:', new_no_sem_total)\n",
    "print('Total of con tags without lex attribute now:', new_no_sem_total)\n",
    "print('Total instances of abstracts truncated at 250 words now:', new_truncated_at_250_words_total)\n",
    "print('Total instances of abstracts truncated at 400 words now:', new_truncated_at_400_words_total)\n",
    "print('Total instances of cons tags with coordinated sem attributes:', new_coordinated_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Lex Attributes\n",
    "Some lex text contains spaces. This is probably due to annotation errors. We replace spaces with underscores and postfix all lexes with the tag \"_lex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WvHL68P3UDwh",
    "outputId": "50a579ed-2775-4787-8420-853f3c28d22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IL-2 receptor alpha\n",
      "IL-2R alpha\n",
      "CD30 ligand\n",
      "proteolytic digestion\n",
      "cytoplasmic component\n",
      "IL-7R alpha\n",
      "IL-7R alpha\n",
      "HSP70B promoter\n"
     ]
    }
   ],
   "source": [
    "# Create data frame of lexes and sems:\n",
    "lex_and_sem = []\n",
    "for con in cons:\n",
    "  lex_and_sem.append(con.attrs)\n",
    "lex_and_sem_df = pd.DataFrame(lex_and_sem)\n",
    "\n",
    "# For any lexes, substitute space characters for underscores and add '_lex' postfix:\n",
    "for i in range(len(lex_and_sem_df['lex'])):\n",
    "    if ' ' in lex_and_sem_df['lex'][i]:\n",
    "        print(lex_and_sem_df['lex'][i])\n",
    "        lex_and_sem_df['lex'][i] = lex_and_sem_df['lex'][i].replace(' ', '_')\n",
    "    lex_and_sem_df['lex'][i] += '_lex'\n",
    "\n",
    "lex_and_sem_df = lex_and_sem_df.drop_duplicates(subset=['lex'])\n",
    "lex_and_sem_df = lex_and_sem_df.reset_index(drop=True)\n",
    "\n",
    "unique_lex = np.array(lex_and_sem_df['lex'])\n",
    "new_lex_col = take_out_str(lex_and_sem_df['lex'], '\\\"')\n",
    "lex_and_sem_df = lex_and_sem_df.replace(lex_and_sem_df['lex'], new_lex_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfoX86nZMpwu"
   },
   "source": [
    "## Output Files Creation\n",
    "Write the three output files:\n",
    "* A CSV file containing the document ID'S and their respective doc index\n",
    "* A TSV file containing a mapping from each lex to each sem\n",
    "* A JSON file containing the preprocessed GENIA Term corpus data. Each element in this JSON file is a list of strings. Some are keywords (if they are in the lex list), some aren't. The words won't be in the same order as the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "LQg2xQJdTCbq",
    "outputId": "a81d4ac5-151b-4afc-ef1e-da265f0116f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MEDLINE:95333264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MEDLINE:95343554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MEDLINE:95347379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MEDLINE:95280913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>MEDLINE:96011839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>MEDLINE:96009598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>MEDLINE:95403454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>MEDLINE:95385995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                ID\n",
       "0         0  MEDLINE:95369245\n",
       "1         1  MEDLINE:95333264\n",
       "2         2  MEDLINE:95343554\n",
       "3         3  MEDLINE:95347379\n",
       "4         4  MEDLINE:95280913\n",
       "...     ...               ...\n",
       "1995   1995  MEDLINE:96011839\n",
       "1996   1996  MEDLINE:96009598\n",
       "1997   1997  MEDLINE:95403454\n",
       "1998   1998  MEDLINE:95385995\n",
       "1999   1999  MEDLINE:95370270\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the text in the bibliomisc tags:\n",
    "doc_id_tags = soup.find_all('bibliomisc')\n",
    "doc_ids = [id.get_text() for id in doc_id_tags]\n",
    "\n",
    "# Write the csv file with the doc ids:\n",
    "csv_outfile_path = '../0-data-preprocessed/GENIAcorpus3.02-doc-ids.csv'\n",
    "index = range(len(doc_ids))\n",
    "doc_ids_df = pd.DataFrame({'index': index, 'ID': doc_ids})\n",
    "doc_ids_df.to_csv(csv_outfile_path, index=False)\n",
    "\n",
    "# File contents summary:\n",
    "display(doc_ids_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL-2_gene_expression_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL-2_gene_lex</td>\n",
       "      <td>G#DNA_domain_or_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF-kappa_B_activation_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF-kappa_B_lex</td>\n",
       "      <td>G#protein_molecule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CD28_lex</td>\n",
       "      <td>G#protein_molecule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31782</th>\n",
       "      <td>gp160-induced_AP-1_complex_lex</td>\n",
       "      <td>G#protein_complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31783</th>\n",
       "      <td>protein_synthesis-independent_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31784</th>\n",
       "      <td>calcium_channel_blocker_lex</td>\n",
       "      <td>G#other_organic_compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31785</th>\n",
       "      <td>anti-CD3-induced_interleukin-2_secretion_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31786</th>\n",
       "      <td>T_cell_unresponsiveness_lex</td>\n",
       "      <td>G#other_name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31787 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                lex                       sem\n",
       "0                          IL-2_gene_expression_lex              G#other_name\n",
       "1                                     IL-2_gene_lex    G#DNA_domain_or_region\n",
       "2                         NF-kappa_B_activation_lex              G#other_name\n",
       "3                                    NF-kappa_B_lex        G#protein_molecule\n",
       "4                                          CD28_lex        G#protein_molecule\n",
       "...                                             ...                       ...\n",
       "31782                gp160-induced_AP-1_complex_lex         G#protein_complex\n",
       "31783             protein_synthesis-independent_lex              G#other_name\n",
       "31784                   calcium_channel_blocker_lex  G#other_organic_compound\n",
       "31785  anti-CD3-induced_interleukin-2_secretion_lex              G#other_name\n",
       "31786                   T_cell_unresponsiveness_lex              G#other_name\n",
       "\n",
       "[31787 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write semantic classes to TSV:\n",
    "genia_keywords_outfile_path = '../0-data-preprocessed/GENIAcorpus3.02-keywords.tsv'\n",
    "lex_and_sem_df.to_csv(genia_keywords_outfile_path, index=False, sep='\\t')\n",
    "\n",
    "# File contents summary:\n",
    "display(lex_and_sem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jurdkpC75H3X"
   },
   "outputs": [],
   "source": [
    "# Create JSON file:\n",
    "xml_articles = soup.find_all('article')\n",
    "\n",
    "joint_descendants = []\n",
    "for article in xml_articles:\n",
    "  joint_descendants.append([descendant for descendant in article.descendants])\n",
    "\n",
    "j_sen = []\n",
    "for i in range(len(joint_descendants)):\n",
    "  j_sen.append([])\n",
    "  for sen in joint_descendants[i]:\n",
    "    if sen.name == 'sentence':\n",
    "      j_sen[i].append(sen)\n",
    "\n",
    "in_sen = []\n",
    "for i in range(len(j_sen)):\n",
    "  in_sen.append([])\n",
    "  for sentence in j_sen[i]:\n",
    "    in_sen[i].append([descendant for descendant in sentence.descendants])\n",
    "\n",
    "in_sentences = []\n",
    "for i in range(len(in_sen)):\n",
    "  in_sentences.append([])\n",
    "  for j in range(len(in_sen[i])):\n",
    "    for k in range(len(in_sen[i][j])):\n",
    "      in_sentences[i].append(in_sen[i][j][k])\n",
    "\n",
    "bag_of_content = []\n",
    "for i in range(len(in_sentences)):\n",
    "  bag_of_content.append([])\n",
    "  for j in range(len(in_sentences[i])):\n",
    "    if type(in_sentences[i][j]) is bs4.element.Tag:\n",
    "      bag_of_content[i].append(in_sentences[i][j].attrs)\n",
    "    elif in_sentences[i][j].parent.name == 'sentence':\n",
    "      bag_of_content[i].append(in_sentences[i][j])\n",
    "\n",
    "keywords = []\n",
    "stopwords = []\n",
    "for i in range(len(bag_of_content)):\n",
    "  keywords.append([])\n",
    "  stopwords.append([])\n",
    "  for j in range(len(bag_of_content[i])):\n",
    "    if type(bag_of_content[i][j]) is dict:\n",
    "        keywords[i].append(bag_of_content[i][j]['lex'])\n",
    "    else:\n",
    "      stopwords[i].append(bag_of_content[i][j])\n",
    "\n",
    "for i in range(len(keywords)):\n",
    "    for j in range(len(keywords[i])):\n",
    "        if ' ' in keywords[i][j]:\n",
    "            keywords[i][j] = keywords[i][j].replace(' ', '_')\n",
    "        keywords[i][j] += '_lex'\n",
    "\n",
    "\n",
    "for i in range(len(stopwords)):\n",
    "  stopwords[i] = normalize(stopwords[i])\n",
    "  stopwords[i] = [word.strip() for word in stopwords[i]]\n",
    "  stopwords[i] = [word for word in stopwords[i] if len(word) > 0]\n",
    "\n",
    "combined = []\n",
    "for i in range(len(stopwords)):\n",
    "  combined.append(keywords[i] + stopwords[i])\n",
    "\n",
    "genia = []\n",
    "for doc in combined:\n",
    "  genia.append(' '.join(doc))\n",
    "\n",
    "take_out_str(genia, '\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "sIlPQEzwQalM",
    "outputId": "3ab07fa4-9e25-45c7-fc8f-ce4dfc0b1d36"
   },
   "outputs": [],
   "source": [
    "# Write JSON to file:\n",
    "genia_terms_outfile_path = '../0-data-preprocessed/GENIAcorpus3.02-preprocessed.json'\n",
    "with open(genia_terms_outfile_path, 'w') as outfile:\n",
    "    json.dump(genia, outfile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
